{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification / tc-nltk-lstm-rnn-10-mix.ipynb\n",
    "# Gourav Siddhad\n",
    "# 27-Mar-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Libraries - Done\n"
     ]
    }
   ],
   "source": [
    "print('Importing Libraries', end='')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, minmax_scale\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents - 10788\n",
      "Extracting (Id, Docs and Labels) - Done\n",
      "Documents -  10788\n",
      "Labels  -  10788\n",
      "Categories -  90\n"
     ]
    }
   ],
   "source": [
    "documents = reuters.fileids()\n",
    "print('Total Documents -', len(documents))\n",
    "\n",
    "print('Extracting (Id, Docs and Labels)', end='')\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "all_docs = train_docs\n",
    "all_docs += test_docs\n",
    "\n",
    "train_labels = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_labels  = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "all_labels = train_labels\n",
    "all_labels += test_labels\n",
    "print(' - Done')\n",
    "\n",
    "del train_docs\n",
    "del test_docs\n",
    "del train_labels\n",
    "del test_labels\n",
    "\n",
    "print('Documents - ', len(all_docs))\n",
    "print('Labels  - ', len(all_labels))\n",
    "\n",
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print('Categories - ', len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching Stop Words - Done\n"
     ]
    }
   ],
   "source": [
    "print('Caching Stop Words', end='')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print(' - Done')\n",
    "\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token), words)))\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token)>=min_length, tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Articles -  10788\n",
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 8400 8500 8600 8700 8800 8900 9000 9100 9200 9300 9400 9500 9600 9700 9800 9900 10000 10100 10200 10300 10400 10500 10600 10700 "
     ]
    }
   ],
   "source": [
    "print('Total Articles - ', len(all_docs))\n",
    "allwords = set()\n",
    "i = 0\n",
    "for doc in all_docs:\n",
    "    if i % 100 is 0:\n",
    "        print(i, end=' ')\n",
    "    doc = tokenize(doc)\n",
    "    for word in doc:\n",
    "        allwords.add(word)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Words -  25170\n"
     ]
    }
   ],
   "source": [
    "print('All Words - ', len(allwords))\n",
    "# print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Train:Test Docs - Done\n",
      "Tokenizing - Done\n"
     ]
    }
   ],
   "source": [
    "print('Sorting Train:Test Docs', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_docs, all_labels, test_size=0.2, random_state=42)\n",
    "print(' - Done')\n",
    "\n",
    "maxwords = len(allwords)\n",
    "\n",
    "print('Tokenizing', end='')\n",
    "tk = Tokenizer(num_words=maxwords)\n",
    "tk.fit_on_texts(X_train)\n",
    "tk.fit_on_texts(X_test)\n",
    "index_list_train = tk.texts_to_sequences(X_train)\n",
    "index_list_test = tk.texts_to_sequences(X_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2379\n",
      "2379\n"
     ]
    }
   ],
   "source": [
    "# max of index_list_train\n",
    "# max of index_list_test\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "for i in index_list_train:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)\n",
    "\n",
    "for i in index_list_test:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxLen -  2379\n",
      "Padding Sequences - Done\n",
      "Binarizing MultiLabels - Done\n"
     ]
    }
   ],
   "source": [
    "# maxlen = 1600\n",
    "print('MaxLen - ', maxlen)\n",
    "print('Padding Sequences', end='')\n",
    "x_train = sequence.pad_sequences(index_list_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(index_list_test, maxlen=maxlen)\n",
    "print(' - Done')\n",
    "\n",
    "print('Binarizing MultiLabels', end='')\n",
    "lb = MultiLabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_docs\n",
    "del all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={\"CPU\":8})\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    with tf.device('/gpu:0'):\n",
    "        inputs = Input(name='inputs', shape=[maxlen])\n",
    "        layer1 = Embedding(maxwords, 1024)(inputs)\n",
    "    with tf.device('/cpu:0'):\n",
    "        layer2 = LSTM(256)(layer1)\n",
    "        layer3 = Dense(128)(layer2)\n",
    "    with tf.device('/gpu:0'):\n",
    "        layer4 = Activation('relu')(layer3)\n",
    "        layer5 = Dropout(rate = 0.1)(layer4) # rate = 1-keep_prob, keep_prob=0.5\n",
    "        layer6 = Dense(len(categories))(layer5)\n",
    "        layer7 = Activation('softmax')(layer6)\n",
    "    model = Model(inputs=inputs, outputs=layer7)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 2379)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2379, 1024)        25774080  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               1311744   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                11610     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 90)                0         \n",
      "=================================================================\n",
      "Total params: 27,130,330\n",
      "Trainable params: 27,130,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6041 samples, validate on 2589 samples\n",
      "Epoch 1/50\n",
      "6041/6041 [==============================] - 11462s 2s/step - loss: 0.0128 - acc: 0.3134 - val_loss: 0.0114 - val_acc: 0.3766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01142, saving model to model-topcat-10-mix-001-0.313359-0.376593.h5\n",
      "Epoch 2/50\n",
      "6041/6041 [==============================] - 15764s 3s/step - loss: 0.0103 - acc: 0.4354 - val_loss: 0.0093 - val_acc: 0.5203\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01142 to 0.00934, saving model to model-topcat-10-mix-002-0.435358-0.520278.h5\n",
      "Epoch 3/50\n",
      "6041/6041 [==============================] - 14103s 2s/step - loss: 0.0091 - acc: 0.5041 - val_loss: 0.0087 - val_acc: 0.5485\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00934 to 0.00870, saving model to model-topcat-10-mix-003-0.504056-0.548474.h5\n",
      "Epoch 4/50\n",
      "6041/6041 [==============================] - 14014s 2s/step - loss: 0.0083 - acc: 0.5347 - val_loss: 0.0082 - val_acc: 0.5585\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00870 to 0.00819, saving model to model-topcat-10-mix-004-0.534680-0.558517.h5\n",
      "Epoch 5/50\n",
      "6041/6041 [==============================] - 14152s 2s/step - loss: 0.0080 - acc: 0.5502 - val_loss: 0.0086 - val_acc: 0.5129\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00819\n",
      "Epoch 6/50\n",
      "6041/6041 [==============================] - 14347s 2s/step - loss: 0.0076 - acc: 0.5779 - val_loss: 0.0080 - val_acc: 0.5651\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00819 to 0.00798, saving model to model-topcat-10-mix-006-0.577884-0.565083.h5\n",
      "Epoch 7/50\n",
      "6041/6041 [==============================] - 14269s 2s/step - loss: 0.0072 - acc: 0.6035 - val_loss: 0.0072 - val_acc: 0.6219\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00798 to 0.00723, saving model to model-topcat-10-mix-007-0.603542-0.621862.h5\n",
      "Epoch 8/50\n",
      "6041/6041 [==============================] - 14062s 2s/step - loss: 0.0068 - acc: 0.6307 - val_loss: 0.0071 - val_acc: 0.6385\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00723 to 0.00709, saving model to model-topcat-10-mix-008-0.630690-0.638470.h5\n",
      "Epoch 9/50\n",
      "6041/6041 [==============================] - 14107s 2s/step - loss: 0.0065 - acc: 0.6515 - val_loss: 0.0071 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00709\n",
      "Epoch 10/50\n",
      "6041/6041 [==============================] - 14179s 2s/step - loss: 0.0073 - acc: 0.6059 - val_loss: 0.0073 - val_acc: 0.6288\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00709\n",
      "Epoch 11/50\n",
      "6041/6041 [==============================] - 14334s 2s/step - loss: 0.0065 - acc: 0.6607 - val_loss: 0.0072 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00709\n",
      "Epoch 12/50\n",
      "6041/6041 [==============================] - 14266s 2s/step - loss: 0.0062 - acc: 0.6749 - val_loss: 0.0071 - val_acc: 0.6466\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00709\n",
      "Epoch 13/50\n",
      "6041/6041 [==============================] - 14298s 2s/step - loss: 0.0061 - acc: 0.6784 - val_loss: 0.0072 - val_acc: 0.6277\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00709\n",
      "Epoch 14/50\n",
      "6041/6041 [==============================] - 14324s 2s/step - loss: 0.0076 - acc: 0.6282 - val_loss: 0.0083 - val_acc: 0.5628\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00709\n",
      "Epoch 15/50\n",
      "6041/6041 [==============================] - 14393s 2s/step - loss: 0.0062 - acc: 0.6855 - val_loss: 0.0066 - val_acc: 0.6686\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00709 to 0.00663, saving model to model-topcat-10-mix-015-0.685483-0.668598.h5\n",
      "Epoch 16/50\n",
      "6041/6041 [==============================] - 14415s 2s/step - loss: 0.0055 - acc: 0.7153 - val_loss: 0.0073 - val_acc: 0.6045\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00663\n",
      "Epoch 17/50\n",
      "6041/6041 [==============================] - 14382s 2s/step - loss: 0.0054 - acc: 0.7154 - val_loss: 0.0064 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00663 to 0.00638, saving model to model-topcat-10-mix-017-0.715444-0.680958.h5\n",
      "Epoch 18/50\n",
      "6041/6041 [==============================] - 14352s 2s/step - loss: 0.0049 - acc: 0.7438 - val_loss: 0.0062 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00638 to 0.00620, saving model to model-topcat-10-mix-018-0.743751-0.698339.h5\n",
      "Epoch 19/50\n",
      "6041/6041 [==============================] - 14439s 2s/step - loss: 0.0048 - acc: 0.7499 - val_loss: 0.0061 - val_acc: 0.6987\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00620 to 0.00610, saving model to model-topcat-10-mix-019-0.749876-0.698725.h5\n",
      "Epoch 20/50\n",
      "6041/6041 [==============================] - 14371s 2s/step - loss: 0.0045 - acc: 0.7649 - val_loss: 0.0061 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00610 to 0.00610, saving model to model-topcat-10-mix-020-0.764940-0.693318.h5\n",
      "Epoch 21/50\n",
      "6041/6041 [==============================] - 14417s 2s/step - loss: 0.0045 - acc: 0.7697 - val_loss: 0.0062 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00610\n",
      "Epoch 22/50\n",
      "6041/6041 [==============================] - 14296s 2s/step - loss: 0.0044 - acc: 0.7686 - val_loss: 0.0062 - val_acc: 0.7030\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00610\n",
      "Epoch 23/50\n",
      "6041/6041 [==============================] - 14352s 2s/step - loss: 0.0045 - acc: 0.7702 - val_loss: 0.0062 - val_acc: 0.6995\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00610\n",
      "Epoch 24/50\n",
      "6041/6041 [==============================] - 14369s 2s/step - loss: 0.0041 - acc: 0.7946 - val_loss: 0.0061 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00610\n",
      "Epoch 25/50\n",
      " 640/6041 [==>...........................] - ETA: 3:21:12 - loss: 0.0038 - acc: 0.7969"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-de467ffcb4d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint = ModelCheckpoint('model-topcat-10-mix-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', \n\u001b[0;32m      2\u001b[0m                              verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model-topcat-10-mix-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', \n",
    "                             verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=50, validation_split=0.3, shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complete Model\n",
    "model.save('tc-nltk-lstm-rnn-10-mix.h5')\n",
    "\n",
    "# # Save Model Configuration to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open('tc-nltk-lstm-rnn-10-mix.json', 'w') as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "# # Load a Saved Model\n",
    "# # model = load_model('tc-nltk-lstm-rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Model Configuration from JSON\n",
    "# json_file = open('tc-nltk-lstm-rnn-topcat-10-mix.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.load_weights('model-10-mix-009-0.917161-0.840018.h5') # Change before running\n",
    "# loaded_model.save('tc-nltk-lstm-rnn-10-mix-weights.hdf5')\n",
    "# loaded_model=load_model('tc-nltk-lstm-rnn-10-mix-weights.hdf5')\n",
    "\n",
    "# Load Best Model\n",
    "loaded_model = load_model('model-topcat-10-mix-020-0.764940-0.693318.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158/2158 [==============================] - 239s 111ms/step\n",
      "\n",
      "Loss: 0.006\tAccuracy: 0.695\n",
      "2158/2158 [==============================] - 16s 7ms/step\n",
      "\n",
      "Loss: 0.006\tAccuracy: 0.674\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr[0], accr[1]))\n",
    "\n",
    "accr2 = loaded_model.evaluate(x_test, y_test, batch_size=256)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr2[0], accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='tc-nltk-lstm-4-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-624aafe04b5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot training & validation accuracy values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training and Validation Accuracy - LSTM RNN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy - LSTM RNN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-10-mix-acc.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation Loss - LSTM RNN')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-10-mix-loss.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
