{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification / tc-nltk-lstm-rnn-10.ipynb\n",
    "# Gourav Siddhad\n",
    "# 27-Mar-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing Libraries', end='')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, minmax_scale\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "print('Total Documents -', len(documents))\n",
    "\n",
    "print('Extracting (Id, Docs and Labels)', end='')\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "all_docs = train_docs\n",
    "all_docs += test_docs\n",
    "\n",
    "train_labels = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_labels  = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "all_labels = train_labels\n",
    "all_labels += test_labels\n",
    "print(' - Done')\n",
    "\n",
    "del train_docs\n",
    "del test_docs\n",
    "del train_labels\n",
    "del test_labels\n",
    "\n",
    "print('Documents - ', len(all_docs))\n",
    "print('Labels  - ', len(all_labels))\n",
    "\n",
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print('Categories - ', len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Caching Stop Words', end='')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print(' - Done')\n",
    "\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token), words)))\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token)>=min_length, tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Articles - ', len(all_docs))\n",
    "allwords = set()\n",
    "i = 0\n",
    "for doc in all_docs:\n",
    "    if i % 100 is 0:\n",
    "        print(i, end=' ')\n",
    "    doc = tokenize(doc)\n",
    "    for word in doc:\n",
    "        allwords.add(word)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Words - ', len(allwords))\n",
    "# print(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorting Train:Test Docs', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_docs, all_labels, test_size=0.2, random_state=42)\n",
    "print(' - Done')\n",
    "\n",
    "maxwords = len(allwords)\n",
    "\n",
    "print('Tokenizing', end='')\n",
    "tk = Tokenizer(num_words=maxwords)\n",
    "tk.fit_on_texts(X_train)\n",
    "tk.fit_on_texts(X_test)\n",
    "index_list_train = tk.texts_to_sequences(X_train)\n",
    "index_list_test = tk.texts_to_sequences(X_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max of index_list_train\n",
    "# max of index_list_test\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "for i in index_list_train:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)\n",
    "\n",
    "for i in index_list_test:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 1600\n",
    "print('MaxLen - ', maxlen)\n",
    "print('Padding Sequences', end='')\n",
    "x_train = sequence.pad_sequences(index_list_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(index_list_test, maxlen=maxlen)\n",
    "print(' - Done')\n",
    "\n",
    "print('Binarizing MultiLabels', end='')\n",
    "lb = MultiLabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_docs\n",
    "del all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[maxlen])\n",
    "    layer1 = Embedding(maxwords, 1024)(inputs)\n",
    "    layer2 = LSTM(256)(layer1)\n",
    "    layer3 = Dense(128)(layer2)\n",
    "    layer4 = Activation('relu')(layer3)\n",
    "    layer5 = Dropout(rate = 0.1)(layer4) # rate = 1-keep_prob, keep_prob=0.5\n",
    "    layer6 = Dense(len(categories))(layer5)\n",
    "    layer7 = Activation('softmax')(layer6)\n",
    "    model = Model(inputs=inputs, outputs=layer7)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-topcat-10-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', \n",
    "                             verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=50, validation_split=0.3, shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complete Model\n",
    "model.save('tc-nltk-lstm-rnn-10.h5')\n",
    "\n",
    "# Save Model Configuration to JSON\n",
    "model_json = model.to_json()\n",
    "with open('tc-nltk-lstm-rnn-10.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Load a Saved Model\n",
    "# model = load_model('tc-nltk-lstm-rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model Configuration from JSON\n",
    "json_file = open('tc-nltk-lstm-rnn-topcat-10.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('model-10-009-0.917161-0.840018.h5') # Change before running\n",
    "loaded_model.save('tc-nltk-lstm-rnn-10-weights.hdf5')\n",
    "loaded_model=load_model('tc-nltk-lstm-rnn-10-weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr[0], accr[1]))\n",
    "\n",
    "accr2 = loaded_model.evaluate(x_test, y_test, batch_size=256)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr2[0], accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='tc-nltk-lstm-4-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy - LSTM RNN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-10-acc.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation Loss - LSTM RNN')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-10-loss.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
