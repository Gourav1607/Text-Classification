{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification / tc-nltk-lstm-rnn-6.ipynb\n",
    "# Gourav Siddhad\n",
    "# 24-Mar-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing Libraries', end='')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, minmax_scale\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "print('Total Documents -', len(documents))\n",
    "\n",
    "print('Extracting (Id, Docs and Labels)', end='')\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "all_docs = train_docs\n",
    "all_docs += test_docs\n",
    "\n",
    "train_labels = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_labels  = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "all_labels = train_labels\n",
    "all_labels += test_labels\n",
    "print(' - Done')\n",
    "\n",
    "del train_docs\n",
    "del test_docs\n",
    "del train_labels\n",
    "del test_labels\n",
    "\n",
    "print('Documents - ', len(all_docs))\n",
    "print('Labels  - ', len(all_labels))\n",
    "\n",
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print('Categories - ', len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Caching Stop Words', end='')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorting Train:Test Docs', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_docs, all_labels, test_size=0.2, random_state=42)\n",
    "print(' - Done')\n",
    "\n",
    "maxwords = 5000\n",
    "\n",
    "print('Tokenizing', end='')\n",
    "tk = Tokenizer(num_words=maxwords)\n",
    "tk.fit_on_texts(X_train)\n",
    "tk.fit_on_texts(X_test)\n",
    "index_list_train = tk.texts_to_sequences(X_train)\n",
    "index_list_test = tk.texts_to_sequences(X_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max of index_list_train\n",
    "# max of index_list_test\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "for i in index_list_train:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)\n",
    "\n",
    "for i in index_list_test:\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1600\n",
    "\n",
    "print('Padding Sequences', end='')\n",
    "x_train = sequence.pad_sequences(index_list_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(index_list_test, maxlen=maxlen)\n",
    "print(' - Done')\n",
    "\n",
    "print('Binarizing MultiLabels', end='')\n",
    "lb = MultiLabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_docs\n",
    "del all_labels\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[maxlen])\n",
    "    layer = Embedding(maxwords, 512, input_length=maxlen)(inputs)\n",
    "    layer = LSTM(256)(layer)\n",
    "    layer = Dense(128, name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(rate = 0.5)(layer) # rate = 1-keep_prob, keep_prob=0.5\n",
    "    layer = Dense(len(categories), name='out_layer')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=256, epochs=20, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complete Model\n",
    "model.save('tc-nltk-lstm-rnn-7.h5')\n",
    "\n",
    "# Load a Saved Model\n",
    "# model = load_model('tc-nltk-lstm-rnn.h5')\n",
    "\n",
    "# Delete a model\n",
    "# del model\n",
    "\n",
    "# Save Model Configuration to JSON\n",
    "model_json = model.to_json()\n",
    "with open('tc-nltk-lstm-rnn-7.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('tc-nltk-lstm-rnn-7-weights.h5')\n",
    "\n",
    "# Load Model Configuration from JSON\n",
    "# json_file = open('tc-nltk-lstm-rnn.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.load_weights('tc-nltk-lstm-rnn-weights.h5')\n",
    "# loaded_model.save('tc-nltk-lstm-rnn-weights.hdf5')\n",
    "# loaded_model=load_model('tc-nltk-lstm-rnn-weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr[0], accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='tc-nltk-lstm-4-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy - LSTM RNN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-7-acc.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation Loss - LSTM RNN')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-7-loss.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
