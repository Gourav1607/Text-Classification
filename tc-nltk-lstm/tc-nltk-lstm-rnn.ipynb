{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification / tc-nltk-lstm-rnn.ipynb\n",
    "# Gourav Siddhad\n",
    "# 16-Mar-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing Libraries', end='')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, minmax_scale, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model, model_from_json\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, SpatialDropout1D, TimeDistributed, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "print(' - Done')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reuters.fileids()\n",
    "print('Total Documents -', len(documents))\n",
    "\n",
    "print('Extracting (Id, Docs and Labels)', end='')\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "all_docs = train_docs\n",
    "all_docs += test_docs\n",
    "\n",
    "train_labels = [reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "test_labels  = [reuters.categories(doc_id) for doc_id in test_docs_id]\n",
    "all_labels = train_labels\n",
    "all_labels += test_labels\n",
    "print(' - Done')\n",
    "\n",
    "del train_docs\n",
    "del test_docs\n",
    "del train_labels\n",
    "del test_labels\n",
    "del train_docs_id\n",
    "del test_docs_id\n",
    "\n",
    "print('Documents - ', len(all_docs))\n",
    "print('Labels  - ', len(all_labels))\n",
    "\n",
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print('Categories - ', len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing', end='')\n",
    "tk = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tk.fit_on_texts(all_docs)\n",
    "index_docs = tk.texts_to_sequences(all_docs)\n",
    "print(' - Done')\n",
    "\n",
    "print('Binarizing MultiLabels', end='')\n",
    "lb = MultiLabelBinarizer()\n",
    "index_labels = lb.fit_transform(all_labels)\n",
    "print(' - Done')\n",
    "\n",
    "print('Sorting Train:Test Docs', end='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(index_docs, index_labels, test_size=0.2, random_state=42)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating Vocabulary', end='')\n",
    "vocab = set()\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "for sent in X_test:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "print(' - Done')\n",
    "vocabulary = len(vocab)\n",
    "\n",
    "del vocab\n",
    "del all_docs\n",
    "del all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Padding Sequences', end='')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=vocabulary)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=vocabulary)\n",
    "print(' - Done')\n",
    "\n",
    "print('Scaling Data to Range', end='')\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_train = mm_scaler.fit_transform(X_train)\n",
    "X_test = mm_scaler.transform(X_test)\n",
    "print(' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "num_steps = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=vocabulary))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(categories)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.20)\n",
    "#, callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complete Model\n",
    "model.save('tc-nltk-lstm-rnn.h5')\n",
    "\n",
    "# Load a Saved Model\n",
    "# model = load_model('tc-nltk-lstm-rnn.h5')\n",
    "\n",
    "# Delete a model\n",
    "# del model\n",
    "\n",
    "# Save Model Configuration to JSON\n",
    "model_json = model.to_json()\n",
    "with open('tc-nltk-lstm-rnn.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('tc-nltk-lstm-rnn-weights.h5')\n",
    "\n",
    "# Load Model Configuration from JSON\n",
    "# json_file = open('tc-nltk-lstm-rnn.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.load_weights('tc-nltk-lstm-rnn-weights.h5')\n",
    "# loaded_model.save('tc-nltk-lstm-rnn-weights.hdf5')\n",
    "# loaded_model=load_model('tc-nltk-lstm-rnn-weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(x_test, y_test)\n",
    "print()\n",
    "print('Loss: {:0.3f}\\tAccuracy: {:0.3f}'.format(accr[0], accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='tc-nltk-lstm-2-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy - LSTM RNN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-acc.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation Loss - LSTM RNN')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('tc-nltk-lstm-rnn-loss.png', dpi=300, pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
